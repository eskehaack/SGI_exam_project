{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eskeh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('stopwords')\n",
    "da_stop = set(nltk.corpus.stopwords.words('danish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = pd.read_csv(\"2_headword_headword_polarity.csv\", header=None)[[0, 4]]\n",
    "sentiment_dict = {row[0]: row[4] for _, row in sent_df.iterrows()}\n",
    "\n",
    "def get_tfs(text):\n",
    "    return dict(nltk.probability.FreqDist(([lemmatizer.lemmatize(w) for word in text if (w:=word.lower()).isalpha()])))\n",
    "\n",
    "def get_sents(text):\n",
    "    tf = get_tfs(nltk.word_tokenize(text))\n",
    "    return (\n",
    "        np.sum([sentiment_dict[word] * tf[word] for word in tf.keys() if word in list(sentiment_dict.keys())]) / \n",
    "        np.sum([tf[word] for word in tf.keys() if word in list(sentiment_dict.keys())]) \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"\"\"\n",
    "    Hvilke hestesteder har du været \"tilknyttet\" , og ca hvornår\n",
    "\n",
    "    Her er en liste for mig\n",
    "\n",
    "    Lejre Rideklub ( ca 1985 - 1990)\n",
    "    Solrød Ridelub ( 1990 -1991 ) - red på en hest der stod opstaldet på den anden side af vejen\n",
    "    Følfods rideklub ( 1991 -1995 )\n",
    "    Gammeltoft rideklub ( 6 måneder sommer 1995)\n",
    "    Adamslyst ridecenter ( 1996 - 1997)\n",
    "    Baunehøj ridecenter ( red der ca 2 år hver anden uge i forbindelse med jeg gik På herlufsholm)\n",
    "    Vallensbæk ridecenter ( 2002-2004)\n",
    "    Dragør store magleby rideklub ( 2006 )\n",
    "\n",
    "    PT\n",
    "    Petersborg ( Høsterkøb) - Søster står der\n",
    "    Og hjemme hos mig selv J\n",
    "\n",
    "    Og så selvfølgelig Enghøjs ponyclub, hvor jeg er kommet siden 1985 og stadig kommer\n",
    "\n",
    "    Jeg har godt nok holdt til Mange steder inden for de sidste 22 år J\n",
    "    Man kan godt \"læse\" at jeg indtil nu udelukkende har været ½ parts rytter J\n",
    "\n",
    "    Hvor har i holdt til??\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Halløj.\n",
    "\n",
    "    Nogen der kan anbefale et sted, hvor hesten kommer på enefold, da vi har brug for den kan være der 1 uge - 3 uger af gangen? Det skal være en radius af 50 km fra Ballerup. Boks er et plus, men løsdrift med fornuftigt læmulighed vil også fungere. Det er til en vallak på 145. Foder og træpiller kan medbringes, men brug for mulighed for at købe wrap.\n",
    "    Dbh\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    \n",
    "    Hej alle sammen.\n",
    "\n",
    "    Jeg går med tanken om at skulle have min egen hest, men har ikke selv råd til at have en med alle udgifterne alene så derfor spørger jeg: er der nogen der har erfaring med det? Har prøvet det en gang og det gik fint, indtil jeg skulle flytte tilbage til Sjælland. Jeg er kun tur rytter og rider ture.\n",
    "\n",
    "    Vh Tine\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Jeg har været i gang med at lede efter et helt specifikt produkt - og blev simpelthen ramt af, hvor besværligt det er at søge noget som helst frem på Facebook.\n",
    "\n",
    "    Prøvede så på HN, men her er søgningen begrænset til ét år tilbage (jeg mener, at den afgrænsning blev indført grundet mængden af data, man kan vist søge sen aften / tidlig morgen, eller hvad ved jeg).\n",
    "\n",
    "    Hvor er det ærgerligt, at HN fik lov at dø. Tilsyneladende er der stadig reklameindtægter på siden, men den bliver simpelthen ikke udviklet længere.\n",
    "\n",
    "    Er der egentlig nogen, der ved hvorfor? Det er jo stadig et site med et stort potentiale, men det er som det bare er blevet efterladt til en stille død. \n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Halløj med jer :-)\n",
    "\n",
    "    Vi skal til 50 års fødselsdag på lørdag. Min mand er pludselig kommet i tanke om en sjov sang, men kan selvfølgelig ikke huske teksten.. Den skulle starte meget fint, men så blive mere og mere mærkelig, med for lange og korte sætninger der ikke rimer og ikke kan synges ;-) Er der nogen af jer der kender den/ved hvor den kan findes? Jeg har prøvet google, men finder desværre kun frem til et link fra 2004 der ikke virker længere.\n",
    "\n",
    "    På forhånd tak.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Hvilke Photo programmer bruger du ? Jeg ville gerne lave nogle flotte billeder med hesten, men ved ikke hvad jeg skal downloade. Hvad bruger i ?\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Hej\n",
    "    Jeg har en del hesteblad som Hestemagasinet, Ridehesten mm. som jeg ikke får læst. Jeg har overvejet at smide dem ud, men synes også det er synd. Tror I der er nogen som kunne være interesseret i dem, og hvor kan de sættes til salg?\n",
    "    \"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013*\"rideklub\" + 0.013*\"ridecenter\" + 0.013*\"godt\"\n",
      "0.013*\"brug\" + 0.013*\"boks\" + 0.013*\"anbefale\"\n",
      "0.062*\"brug\" + 0.033*\"kommer\" + 0.033*\"nogen\"\n",
      "0.013*\"brug\" + 0.013*\"boks\" + 0.013*\"anbefale\"\n",
      "0.013*\"brug\" + 0.013*\"boks\" + 0.013*\"anbefale\"\n",
      "0.013*\"brug\" + 0.013*\"boks\" + 0.013*\"anbefale\"\n",
      "0.013*\"brug\" + 0.013*\"enefold\" + 0.013*\"medbringes\"\n",
      "0.013*\"rideklub\" + 0.013*\"ridecenter\" + 0.013*\"holdt\"\n",
      "0.013*\"brug\" + 0.013*\"rideklub\" + 0.013*\"vejen\"\n",
      "0.062*\"rideklub\" + 0.047*\"ridecenter\" + 0.032*\"godt\"\n"
     ]
    }
   ],
   "source": [
    "def clean_text(texts):\n",
    "    return [\n",
    "        [\n",
    "            w for word in nltk.word_tokenize(text) \n",
    "            if not (len(word) < 4 or word in da_stop) \n",
    "            and (w:=lemmatizer.lemmatize(word).lower()).isalpha()\n",
    "        ] \n",
    "        for text in texts\n",
    "    ]\n",
    "\n",
    "clt = clean_text(texts)\n",
    "dictionary = corpora.Dictionary(clt)\n",
    "corpus = [dictionary.doc2bow(text) for text in clt]\n",
    "\n",
    "import gensim\n",
    "NUM_TOPICS = 10\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "topics = ldamodel.print_topics(num_words=3)\n",
    "print(\"\\n\".join([topic[1] for topic in topics]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kommet, selvfølgelig, prøvet, længere\n",
      "hest, rytter, indtil, flytte\n",
      "mere, tanke, huske, korte\n",
      "ridehesten, hesteblad, overvejet, smide\n",
      "søge, simpelthen, tilbage, stadig\n",
      "søge, simpelthen, stadig, site\n",
      "brug, læmulighed, ballerup, medbringes\n",
      "rideklub, ridecenter, holdt, godt\n",
      "hestemagasinet, synd, synes, smide\n",
      "rideklub, ridecenter, godt, holdt\n",
      "kommer, rideklub, gammeltoft, søster\n",
      "bruger, lave, billeder, programmer\n",
      "kommer, hesten, boks, anbefale\n",
      "siden, stadig, aften, holdt\n",
      "hesteblad, sættes, interesseret, salg\n",
      "ridecenter, stod, udelukkende, selvfølgelig\n",
      "fungere, wrap, brug, mulighed\n",
      "simpelthen, morgen, indført, afgrænsning\n",
      "tror, overvejet, synes, smide\n",
      "fint, prøvet, ture, sammen\n",
      "bruger, gerne, flotte, photo\n",
      "hesten, kommer, løsdrift, anbefale\n",
      "mere, finder, mærkelig, sang\n",
      "selvfølgelig, kommet, forhånd, sjov\n",
      "mere, virker, link, rimer\n",
      "indtil, rytter, rider, derfor\n",
      "hestesteder, godt, baunehøj, side\n",
      "egen, udgifterne, tine, sammen\n",
      "specifikt, grundet, hvorfor, besværligt\n",
      "hesten, bruger, downloade, photo\n",
      "mere, mand, sjov, korte\n",
      "nogen, efterladt, afgrænsning, stille\n",
      "efterladt, afgrænsning, stille, morgen\n",
      "nogen, site, ærgerligt, hestemagasinet\n",
      "løsdrift, anbefale, købe, foder\n",
      "sommer, hest, gammeltoft, store\n",
      "efterladt, afgrænsning, stille, morgen\n",
      "sættes, synes, læst, smide\n",
      "google, sætninger, halløj, link\n",
      "nogen, ærgerligt, hesten, hvornår\n",
      "nogen, kommer, uger, løsdrift\n",
      "steder, udelukkende, store, rideklub\n",
      "efterladt, afgrænsning, stille, søge\n",
      "opstaldet, steder, rytter, siden\n",
      "bruger, downloade, photo, programmer\n",
      "photo, hvilke, flotte, bruger\n",
      "nogen, aften, indført, grundet\n",
      "bare, facebook, grundet, frem\n",
      "træpiller, uger, løsdrift, wrap\n",
      "mere, link, mærkelig, teksten\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def clean_text(texts):\n",
    "    return [\n",
    "        ' '.join(\n",
    "            [\n",
    "                w for word in nltk.word_tokenize(text) \n",
    "                if not (len(word) < 4 or word in da_stop) \n",
    "                and (w:=lemmatizer.lemmatize(word).lower()).isalpha()\n",
    "            ]\n",
    "        )\n",
    "        for text in texts\n",
    "    ]\n",
    "\n",
    "document = clean_text(texts=texts)\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, min_df=1, stop_words=list(da_stop))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(document)\n",
    "\n",
    "# Apply Non-Negative Matrix Factorization (NMF) for topic modeling\n",
    "num_topics = 50  # You can adjust the number of topics as needed\n",
    "nmf = NMF(n_components=num_topics, random_state=1)\n",
    "nmf.fit(tfidf_matrix)\n",
    "\n",
    "# Print the top words for each topic\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    top_words_idx = topic.argsort()[:-5:-1]  # Get the indices of the top 4 words for each topic\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(', '.join(top_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
